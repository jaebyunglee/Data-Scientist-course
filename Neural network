##########################################################################
##### Google - anaconda - python 3.6 version download - all check!!!!#####
##########################################################################

#installation
install.packages("devtools")
library(devtools)
devtools::install_github("rstudio/tensorflow")
library(tensorflow)
install_tensorflow()

#data(download)
alldata <- read.table("C:\\Users\\USER\\Desktop\\R怨듬?\\?븰?쉶 ?뿬由꾪븰援?\\buytest.txt",sep='\t',header=T)
head(alldata,10)

ckall <- complete.cases(alldata[,c("RESPOND","AGE","FICO","BUY18")])
data <-alldata[ckall,]

### you must be scale
data$AGE <- scale(data$AGE)
data$FICO <- scale(data$FICO)

####################################################
################## Neural Network ##################
####################################################
install.packages("neuralnet")
library(neuralnet)

set.seed(123)
fit.nn1 <- neuralnet(RESPOND~AGE+FICO+BUY18, data=data,hidden=2,stepmax = 1e+04, threshold = 0.05,act.fct='logistic') 
print(fit.nn1)
summary(fit.nn1)
print(fit.nn1$weights)
plot(fit.nn1)

#data split
n <- nrow(data)
ck <- sort(sample.int(n, round(n*0.7,0),replace = F))
train <- data[ck,]
test <- data[-ck,]
##########################################################
#######comparison logit and neural network################
##########################################################
set.seed(123)
#logit
fit.logit <- glm(RESPOND~AGE+FICO+BUY18,data=train,family="binomial")
prob <- predict(fit.logit,type="response")
yhat.logit <- ifelse(prob > 0.5,1,0)
table(train$RESPOND,yhat.logit,dnn=c("Observed","Expected"))

#neural network
fit.nn2 <- neuralnet(RESPOND~AGE+FICO+BUY18,data=train, hidden=c(2,2),
                     stepmax = 1e+04, threshold = 0.05, act.fct='logistic')
plot(fit.nn2)
prob.nn2 <- compute(fit.nn2,covariate=train[,c("AGE","FICO","BUY18")])  #<------ i don't know......
pred.nn2 <- ifelse(prob.nn2$net.result > 0.5,1,0)
table(train$RESPOND,pred.nn2,dnn=c("Observed","Expected"))
prob.test <- compute(fit.nn2,covariate=test[,c("AGE","FICO","BUY18")]) 

#################################################################
############################## CNN ##############################
#################################################################
#### Multilayer ConvNet

# Weight Initialization
weight_variable <- function(shape) {
  initial <- tf$truncated_normal(shape, stddev=0.1)
  tf$Variable(initial)
}

bias_variable <- function(shape) {
  initial <- tf$constant(0.1, shape=shape)
  tf$Variable(initial)
}

# Convolution and Pooling
conv2d <- function(x, W) {
  tf$nn$conv2d(x, W, strides=c(1L, 1L, 1L, 1L), padding='SAME')  
}  # 

max_pool_2x2 <- function(x) {
  tf$nn$max_pool(
    x, 
    ksize=c(1L, 2L, 2L, 1L),
    strides=c(1L, 2L, 2L, 1L), 
    padding='SAME')
}

##########################################

### MNIST
#data
input_dataset <- tf$examples$tutorials$mnist$input_data
mnist <- input_dataset$read_data_sets("MNIST-data", one_hot = TRUE)


########### Placeholders ##############
x <- tf$placeholder(tf$float32, shape(NULL, 784L))
y <- tf$placeholder(tf$float32, shape(NULL, 10L))
# Variables
W <- tf$Variable(tf$zeros(shape(784L, 10L)))
b <- tf$Variable(tf$zeros(shape(10L)))

### First Convolutional Layer 
W_conv1 <- weight_variable(shape(5L, 5L, 1L, 32L))
# 5 X 5 kernel size, 1 color, 32 activation maps 
b_conv1 <- bias_variable(shape(32L))
x_image <- tf$reshape(x, shape(-1L, 28L, 28L, 1L))  # 28 X 28 = 784

h_conv1 <- tf$nn$relu(conv2d(x_image, W_conv1) + b_conv1)
h_pool1 <- max_pool_2x2(h_conv1)  # 14 X 14

### Second Convolutional Layer
W_conv2 <- weight_variable(shape = shape(5L, 5L, 32L, 64L))
b_conv2 <- bias_variable(shape = shape(64L))
h_conv2 <- tf$nn$relu(conv2d(h_pool1, W_conv2) + b_conv2)
h_pool2 <- max_pool_2x2(h_conv2)  # 7 X 7

### Densely Connected Layer
W_fc1 <- weight_variable(shape(7L * 7L * 64L, 1024L))
b_fc1 <- bias_variable(shape(1024L))

h_pool2_flat <- tf$reshape(h_pool2, shape(-1L, 7L * 7L * 64L))
h_fc1 <- tf$nn$relu(tf$matmul(h_pool2_flat, W_fc1) + b_fc1)

# Dropout
keep_prob <- tf$placeholder(tf$float32)
h_fc1_drop <- tf$nn$dropout(h_fc1, keep_prob)

# Readout Layer
W_fc2 <- weight_variable(shape(1024L, 10L))
b_fc2 <- bias_variable(shape(10L))

# Prediction & Loss function
mu <- tf$nn$softmax(tf$matmul(h_fc1_drop, W_fc2) + b_fc2)
cross_entropy <- tf$reduce_mean(-tf$reduce_sum(y * tf$log(mu), reduction_indices=1L))
# optimization
train_step <- tf$train$AdamOptimizer(1e-4)$minimize(cross_entropy)
## evalute the model
correct_prediction <- tf$equal(tf$argmax(mu, 1L), tf$argmax(y, 1L))
accuracy <- tf$reduce_mean(tf$cast(correct_prediction, tf$float32))


sess <- tf$InteractiveSession()
sess$run(tf$global_variables_initializer())

# for (i in 1:20000)
for (i in 1:1000) {
  batch <- mnist$train$next_batch(50L)
  if (i %% 50 == 0) {
    train_accuracy <- accuracy$eval(feed_dict = dict(
      x = batch[[1]], y = batch[[2]], keep_prob = 1.0))
    cat(sprintf("step %d, training accuracy %g\n", i, train_accuracy))
  }
  train_step$run(feed_dict = dict(
    x = batch[[1]], y = batch[[2]], keep_prob = 0.5))
}

test_accuracy <- accuracy$eval(feed_dict = dict(
  x = mnist$test$images, y = mnist$test$labels, keep_prob = 1.0))
cat(sprintf("test accuracy %g", test_accuracy))
